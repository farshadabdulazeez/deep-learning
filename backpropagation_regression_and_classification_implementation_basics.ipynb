{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOf7t4+//I1+vOEA8q+Jh7+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farshadabdulazeez/deep-learning-basics/blob/main/backpropagation_regression_and_classification_implementation_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Backpropagation in Regression***"
      ],
      "metadata": {
        "id": "s2oHywVphuQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Libraries and Dataset Creation**"
      ],
      "metadata": {
        "id": "6ujB6LM_cws1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tLA_MP1kcKkB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Creating a dataset with CGPA, Profile Score, and LPA (Lakhs Per Annum)\n",
        "df = pd.DataFrame([[8,8,4],\n",
        "                   [7,9,5],\n",
        "                   [6,10,6],\n",
        "                   [5,12,7]],\n",
        "                  columns=['cgpa', 'profile_score', 'lpa'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parameter Initialization**"
      ],
      "metadata": {
        "id": "ycDuaAzRc9Je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(layer_dims):\n",
        "    \"\"\"\n",
        "    Initialize the parameters (weights and biases) for a neural network.\n",
        "\n",
        "    Args:\n",
        "    layer_dims -- List containing the dimensions of each layer in the network\n",
        "\n",
        "    Returns:\n",
        "    parameters -- Dictionary containing initialized weights and biases\n",
        "    \"\"\"\n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.ones((layer_dims[l-1], layer_dims[l])) * 0.1\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "\n",
        "    return parameters\n",
        "\n",
        "# Example Usage\n",
        "parameters = initialize_parameters([2, 2, 1])  # 2-input, 2-hidden, 1-output\n"
      ],
      "metadata": {
        "id": "tImWSXnRcv0D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Linear Forward Function**"
      ],
      "metadata": {
        "id": "iMqX5TiedBCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_forward(A_prev, W, b):\n",
        "    \"\"\"\n",
        "    Compute the linear part of the forward propagation.\n",
        "\n",
        "    Args:\n",
        "    A_prev -- Activations from the previous layer\n",
        "    W -- Weights matrix of the current layer\n",
        "    b -- Bias vector of the current layer\n",
        "\n",
        "    Returns:\n",
        "    Z -- Linear combination of inputs (W.T * A_prev + b)\n",
        "    \"\"\"\n",
        "    Z = np.dot(W.T, A_prev) + b\n",
        "    return Z\n"
      ],
      "metadata": {
        "id": "kJglTysOc5O5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Forward Propagation for the Full Network**"
      ],
      "metadata": {
        "id": "uRh7OVq5dNjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def L_layer_forward(X, parameters):\n",
        "    \"\"\"\n",
        "    Forward propagation for the entire network.\n",
        "\n",
        "    Args:\n",
        "    X -- Input data\n",
        "    parameters -- Dictionary containing weights and biases\n",
        "\n",
        "    Returns:\n",
        "    A -- Final output of the network\n",
        "    A_prev -- Activations from the second-last layer\n",
        "    \"\"\"\n",
        "\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of layers in the neural network\n",
        "\n",
        "    for l in range(1, L+1):\n",
        "      A_prev = A\n",
        "      Wl = parameters['W' + str(l)]\n",
        "      bl = parameters['b' + str(l)]\n",
        "      #print(\"A\"+str(l-1)+\": \", A_prev)\n",
        "      #print(\"W\"+str(l)+\": \", Wl)\n",
        "      #print(\"b\"+str(l)+\": \", bl)\n",
        "      #print(\"--\"*20)\n",
        "\n",
        "      A = linear_forward(A_prev, Wl, bl)\n",
        "      #print(\"A\"+str(l)+\": \", A)\n",
        "      #print(\"**\"*20)\n",
        "\n",
        "    return A,A_prev\n"
      ],
      "metadata": {
        "id": "YwTy7XDudJxp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Update Parameters (Gradient Update)**"
      ],
      "metadata": {
        "id": "prV3PiK_dVw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(parameters, y, y_hat, A1, X):\n",
        "    \"\"\"\n",
        "    Update weights and biases using gradient descent.\n",
        "\n",
        "    Args:\n",
        "    parameters -- Current weights and biases\n",
        "    y -- True label\n",
        "    y_hat -- Predicted value\n",
        "    A1 -- Activations from the hidden layer\n",
        "    X -- Input features\n",
        "    \"\"\"\n",
        "    parameters['W2'][0][0] += 0.001 * 2 * (y - y_hat) * A1[0][0]\n",
        "    parameters['W2'][1][0] += 0.001 * 2 * (y - y_hat) * A1[1][0]\n",
        "    parameters['b2'][0][0] += 0.001 * 2 * (y - y_hat)\n",
        "\n",
        "    parameters['W1'][0][0] += 0.001 * 2 * (y - y_hat) * parameters['W2'][0][0] * X[0][0]\n",
        "    parameters['W1'][0][1] += 0.001 * 2 * (y - y_hat) * parameters['W2'][0][0] * X[1][0]\n",
        "    parameters['b1'][0][0] += 0.001 * 2 * (y - y_hat) * parameters['W2'][0][0]\n",
        "\n",
        "    parameters['W1'][1][0] += 0.001 * 2 * (y - y_hat) * parameters['W2'][1][0] * X[0][0]\n",
        "    parameters['W1'][1][1] += 0.001 * 2 * (y - y_hat) * parameters['W2'][1][0] * X[1][0]\n",
        "    parameters['b1'][1][0] += 0.001 * 2 * (y - y_hat) * parameters['W2'][1][0]\n"
      ],
      "metadata": {
        "id": "DGBHkveqdP_X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training the Network**"
      ],
      "metadata": {
        "id": "scDrDPtedcml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['cgpa', 'profile_score']].values[0].reshape(2, 1)\n",
        "y = df[['lpa']].values[0][0]\n",
        "\n",
        "# Forward Propagation\n",
        "y_hat, A1 = L_layer_forward(X, parameters)\n",
        "y_hat = y_hat[0][0]\n",
        "\n",
        "# Update Parameters\n",
        "update_parameters(parameters, y, y_hat, A1, X)\n"
      ],
      "metadata": {
        "id": "5SN-Red2dYWe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Full Training with Epochs**"
      ],
      "metadata": {
        "id": "rNaxrVuUdl4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Parameters\n",
        "parameters = initialize_parameters([2, 2, 1])\n",
        "epochs = 5\n",
        "\n",
        "for i in range(epochs):\n",
        "    Loss = []\n",
        "\n",
        "    for j in range(df.shape[0]):\n",
        "        X = df[['cgpa', 'profile_score']].values[j].reshape(2, 1)\n",
        "        y = df[['lpa']].values[j][0]\n",
        "\n",
        "        # Forward Propagation\n",
        "        y_hat, A1 = L_layer_forward(X, parameters)\n",
        "        y_hat = y_hat[0][0]\n",
        "\n",
        "        # Update Parameters\n",
        "        update_parameters(parameters, y, y_hat, A1, X)\n",
        "\n",
        "        # Calculate Loss\n",
        "        Loss.append((y - y_hat) ** 2)\n",
        "\n",
        "    # Print Loss per Epoch\n",
        "    print('Epoch -', i+1, 'Loss -', np.array(Loss).mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkCAddPrdlgS",
        "outputId": "271162cc-d8c4-4eef-b856-c2e682966b09"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 1 Loss - 26.28249792398698\n",
            "Epoch - 2 Loss - 19.438253848220803\n",
            "Epoch - 3 Loss - 10.139874435827522\n",
            "Epoch - 4 Loss - 3.385561305106485\n",
            "Epoch - 5 Loss - 1.3198454128484565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwpf37MtdgEG",
        "outputId": "85a1cb85-8248-4ba6-9665-2f02062c3148"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W1': array([[0.273603  , 0.3993222 ],\n",
              "        [0.28787155, 0.42586102]]),\n",
              " 'b1': array([[0.02885522],\n",
              "        [0.03133223]]),\n",
              " 'W2': array([[0.42574893],\n",
              "        [0.50219328]]),\n",
              " 'b2': array([[0.11841278]])}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Backpropagation in Classification***"
      ],
      "metadata": {
        "id": "HTEO00SnhoDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Create dataset\n",
        "df = pd.DataFrame([[8, 8, 1], [7, 9, 1], [6, 10, 0], [5, 5, 0]],\n",
        "                  columns=['cgpa', 'profile_score', 'placed'])"
      ],
      "metadata": {
        "id": "wrEDAxoqdt8F"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to initialize parameters for the neural network\n",
        "def initialize_parameters(layer_dims):\n",
        "    \"\"\"\n",
        "    Initialize the weights and biases for each layer of the neural network.\n",
        "    Args:\n",
        "        layer_dims (list): List containing dimensions of each layer in the network.\n",
        "    Returns:\n",
        "        parameters (dict): Dictionary containing initialized weights and biases.\n",
        "    \"\"\"\n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)  # Number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.ones((layer_dims[l-1], layer_dims[l])) * 0.1\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "igZ6IW98hzjr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function for forward propagation for a single layer\n",
        "def linear_forward(A_prev, W, b):\n",
        "    \"\"\"\n",
        "    Perform forward propagation for a single layer.\n",
        "    Args:\n",
        "        A_prev (numpy.ndarray): Activations from the previous layer.\n",
        "        W (numpy.ndarray): Weights for the current layer.\n",
        "        b (numpy.ndarray): Biases for the current layer.\n",
        "    Returns:\n",
        "        A (numpy.ndarray): Activation for the current layer.\n",
        "    \"\"\"\n",
        "    Z = np.dot(W.T, A_prev) + b\n",
        "    A = sigmoid(Z)\n",
        "    return A"
      ],
      "metadata": {
        "id": "FtEGuSEyh3pa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for sigmoid activation\n",
        "def sigmoid(Z):\n",
        "    \"\"\"\n",
        "    Apply sigmoid activation function.\n",
        "    Args:\n",
        "        Z (numpy.ndarray): Linear activation input.\n",
        "    Returns:\n",
        "        A (numpy.ndarray): Sigmoid output.\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-Z))"
      ],
      "metadata": {
        "id": "HdH25EPhh6Kp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward propagation for the entire network\n",
        "def L_layer_forward(X, parameters):\n",
        "    \"\"\"\n",
        "    Perform forward propagation through the entire network.\n",
        "    Args:\n",
        "        X (numpy.ndarray): Input data.\n",
        "        parameters (dict): Dictionary containing weights and biases.\n",
        "    Returns:\n",
        "        A (numpy.ndarray): Output of the final layer.\n",
        "        A_prev (numpy.ndarray): Activation from the last hidden layer.\n",
        "    \"\"\"\n",
        "    A = X\n",
        "    L = len(parameters) // 2  # Number of layers in the network\n",
        "\n",
        "    for l in range(1, L+1):\n",
        "        A_prev = A\n",
        "        Wl = parameters['W' + str(l)]\n",
        "        bl = parameters['b' + str(l)]\n",
        "        A = linear_forward(A_prev, Wl, bl)\n",
        "\n",
        "    return A, A_prev"
      ],
      "metadata": {
        "id": "84dFqHfBh8Mp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to update parameters using gradient descent\n",
        "def update_parameters(parameters, y, y_hat, A1, X):\n",
        "    \"\"\"\n",
        "    Update the weights and biases based on the error and gradients.\n",
        "    Args:\n",
        "        parameters (dict): Dictionary containing weights and biases.\n",
        "        y (int): True label for the current training example.\n",
        "        y_hat (float): Predicted output for the current training example.\n",
        "        A1 (numpy.ndarray): Activation from the first hidden layer.\n",
        "        X (numpy.ndarray): Input data for the current training example.\n",
        "    \"\"\"\n",
        "    parameters['W2'][0][0] += 0.0001 * (y - y_hat) * A1[0][0]\n",
        "    parameters['W2'][1][0] += 0.0001 * (y - y_hat) * A1[1][0]\n",
        "    parameters['b2'][0][0] += 0.0001 * (y - y_hat)\n",
        "\n",
        "    parameters['W1'][0][0] += 0.0001 * (y - y_hat) * parameters['W2'][0][0] * A1[0][0] * (1-A1[0][0]) * X[0][0]\n",
        "    parameters['W1'][0][1] += 0.0001 * (y - y_hat) * parameters['W2'][0][0] * A1[0][0] * (1-A1[0][0]) * X[1][0]\n",
        "    parameters['b1'][0][0] += 0.0001 * (y - y_hat) * parameters['W2'][0][0] * A1[0][0] * (1-A1[0][0])\n",
        "\n",
        "    parameters['W1'][1][0] += 0.0001 * (y - y_hat) * parameters['W2'][1][0] * A1[1][0] * (1-A1[1][0]) * X[0][0]\n",
        "    parameters['W1'][1][1] += 0.0001 * (y - y_hat) * parameters['W2'][1][0] * A1[1][0] * (1-A1[1][0]) * X[1][0]\n",
        "    parameters['b1'][1][0] += 0.0001 * (y - y_hat) * parameters['W2'][1][0] * A1[1][0] * (1-A1[1][0])"
      ],
      "metadata": {
        "id": "RAQi_emyiAXh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training with epochs\n",
        "parameters = initialize_parameters([2, 2, 1])  # Initialize parameters for 2-2-1 network\n",
        "epochs = 50  # Number of epochs for training\n",
        "\n",
        "for i in range(epochs):\n",
        "    Loss = []\n",
        "\n",
        "    # Iterate through each example in the dataset\n",
        "    for j in range(df.shape[0]):\n",
        "        X = df[['cgpa', 'profile_score']].values[j].reshape(2, 1)  # Input features\n",
        "        y = df[['placed']].values[j][0]  # True label\n",
        "\n",
        "        y_hat, A1 = L_layer_forward(X, parameters)  # Forward propagation\n",
        "        y_hat = y_hat[0][0]\n",
        "\n",
        "        update_parameters(parameters, y, y_hat, A1, X)  # Update weights and biases\n",
        "\n",
        "        # Compute loss for the current example\n",
        "        Loss.append(-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat))\n",
        "\n",
        "    # Print the average loss for the epoch\n",
        "    print('Epoch -', i+1, 'Loss -', np.array(Loss).mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdW3DNJliDhh",
        "outputId": "db4dba88-2825-4065-d279-0ef0305cdf7c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 1 Loss - 0.6939124309994983\n",
            "Epoch - 2 Loss - 0.6939114509742006\n",
            "Epoch - 3 Loss - 0.6939104713855384\n",
            "Epoch - 4 Loss - 0.6939094922333021\n",
            "Epoch - 5 Loss - 0.6939085135172819\n",
            "Epoch - 6 Loss - 0.6939075352372682\n",
            "Epoch - 7 Loss - 0.6939065573930516\n",
            "Epoch - 8 Loss - 0.6939055799844227\n",
            "Epoch - 9 Loss - 0.6939046030111722\n",
            "Epoch - 10 Loss - 0.693903626473091\n",
            "Epoch - 11 Loss - 0.6939026503699699\n",
            "Epoch - 12 Loss - 0.6939016747016002\n",
            "Epoch - 13 Loss - 0.6939006994677729\n",
            "Epoch - 14 Loss - 0.6938997246682792\n",
            "Epoch - 15 Loss - 0.6938987503029107\n",
            "Epoch - 16 Loss - 0.6938977763714584\n",
            "Epoch - 17 Loss - 0.6938968028737142\n",
            "Epoch - 18 Loss - 0.6938958298094697\n",
            "Epoch - 19 Loss - 0.6938948571785166\n",
            "Epoch - 20 Loss - 0.6938938849806471\n",
            "Epoch - 21 Loss - 0.6938929132156526\n",
            "Epoch - 22 Loss - 0.6938919418833256\n",
            "Epoch - 23 Loss - 0.693890970983458\n",
            "Epoch - 24 Loss - 0.6938900005158424\n",
            "Epoch - 25 Loss - 0.6938890304802711\n",
            "Epoch - 26 Loss - 0.6938880608765363\n",
            "Epoch - 27 Loss - 0.6938870917044309\n",
            "Epoch - 28 Loss - 0.6938861229637474\n",
            "Epoch - 29 Loss - 0.6938851546542789\n",
            "Epoch - 30 Loss - 0.6938841867758179\n",
            "Epoch - 31 Loss - 0.6938832193281578\n",
            "Epoch - 32 Loss - 0.6938822523110912\n",
            "Epoch - 33 Loss - 0.6938812857244119\n",
            "Epoch - 34 Loss - 0.6938803195679129\n",
            "Epoch - 35 Loss - 0.6938793538413873\n",
            "Epoch - 36 Loss - 0.6938783885446289\n",
            "Epoch - 37 Loss - 0.6938774236774317\n",
            "Epoch - 38 Loss - 0.693876459239589\n",
            "Epoch - 39 Loss - 0.6938754952308946\n",
            "Epoch - 40 Loss - 0.6938745316511425\n",
            "Epoch - 41 Loss - 0.6938735685001267\n",
            "Epoch - 42 Loss - 0.6938726057776414\n",
            "Epoch - 43 Loss - 0.6938716434834808\n",
            "Epoch - 44 Loss - 0.6938706816174391\n",
            "Epoch - 45 Loss - 0.693869720179311\n",
            "Epoch - 46 Loss - 0.6938687591688908\n",
            "Epoch - 47 Loss - 0.6938677985859731\n",
            "Epoch - 48 Loss - 0.693866838430353\n",
            "Epoch - 49 Loss - 0.6938658787018248\n",
            "Epoch - 50 Loss - 0.6938649194001839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters  # Final trained parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQYXe7msiIqp",
        "outputId": "0b7c4003-9559-4259-9fb4-7061f77547e7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W1': array([[0.09999042, 0.09990338],\n",
              "        [0.09999049, 0.0999034 ]]),\n",
              " 'b1': array([[-2.63694652e-05],\n",
              "        [-2.63678488e-05]]),\n",
              " 'W2': array([[0.09960344],\n",
              "        [0.09960349]]),\n",
              " 'b2': array([[-0.00080196]])}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}